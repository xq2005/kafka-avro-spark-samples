/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package data_generator_simulator;

import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import org.apache.kafka.clients.producer.Callback;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import io.confluent.kafka.serializers.KafkaAvroSerializerConfig;
import io.confluent.kafka.serializers.KafkaAvroSerializer;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.io.Reader;
import java.nio.file.Paths;
import java.util.Properties;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicLong;
import java.nio.file.Files;

import com.opencsv.CSVReader;
import com.opencsv.CSVReaderBuilder;
import com.opencsv.exceptions.CsvBadConverterException;

public class ConfluentAvroProducer {
    private static Logger log = LoggerFactory.getLogger(ConfluentAvroProducer.class.getName());
    
    public static void main(String[] args) {
       
        String KAFKA_TOPIC = SampleUtils.setStringParameters("KAFKA_TOPIC","ce_online_retail_II");
        int TOTAL_RECORDS = SampleUtils.setIntegerParameters("TOTAL_RECORDS", 100);
        String BOOTSTRAP_SERVERS = SampleUtils.setStringParameters("BOOTSTRAP_SERVERS", "kafka:9092");
        String SCHEMA_REGISTRY_URL = SampleUtils.setStringParameters("SCHEMA_REGISTRY_URL", "http://schema-registry:8081");
        String SCHEMA_PATH = SampleUtils.setStringParameters("SCHEMA_PATH", "/input_data/online_retail_II_serialize_schema.json");
        String CSV_FILE = SampleUtils.setStringParameters("CSV_FILE", "/input_data//online_retail_II.csv");

        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());
        props.put(KafkaAvroSerializerConfig.SCHEMA_REGISTRY_URL_CONFIG, SCHEMA_REGISTRY_URL);
        props.put(ProducerConfig.ACKS_CONFIG, "1");

        log.info("Topic Name: {}, Total Records: {}, Bootstrap Servers: {}, Schema Registry URL: {}",
                KAFKA_TOPIC, TOTAL_RECORDS, BOOTSTRAP_SERVERS, SCHEMA_REGISTRY_URL);

        final KafkaProducer<GenericRecord, GenericRecord> producer = new KafkaProducer<>(props);
        Runtime.getRuntime().addShutdownHook(new Thread(producer::close, "Shutdown-thread"));

        
        String keySchemaString = "{\"type\": \"record\",\"name\": \"key\",\"fields\":[{\"type\": \"string\",\"name\": \"key\"}]}}";
        String valueSchemaString = "";

        try {
            valueSchemaString = new String(Files.readAllBytes(Paths.get(SCHEMA_PATH)));
        } catch (IOException e) {
            e.printStackTrace();
            System.exit(1);
        }

        Schema avroKeySchema = new Schema.Parser().parse(keySchemaString);
        Schema avroValueSchema = new Schema.Parser().parse(valueSchemaString);

        AtomicLong errorCount = new AtomicLong();
        CountDownLatch requestLatch = new CountDownLatch(TOTAL_RECORDS);

        final AtomicLong successCount = new AtomicLong();

        Callback postSender = (recordMetadata, e) -> {
            if (e != null) {
                log.error("Error adding to topic", e);
                errorCount.incrementAndGet();
            } else {
                successCount.incrementAndGet();
            }
            requestLatch.countDown();
        };

        ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);
        scheduler.scheduleAtFixedRate(
                () -> log.info("Successfully created {} Kafka records", successCount.get()),
                2, SampleUtils.PROGRESS_REPORTING_INTERVAL, TimeUnit.SECONDS);

        
        int total_read_lines = 0;
        try (Reader file_reader = Files.newBufferedReader(Paths.get(CSV_FILE));
            CSVReader csv_reader = new CSVReaderBuilder(file_reader).withSkipLines(1).build()) {
                String[] record;
                while (total_read_lines < TOTAL_RECORDS && (record = csv_reader.readNext()) != null) {
                    GenericRecord thisKeyRecord = new GenericData.Record(avroKeySchema);
                    GenericRecord thisValueRecord = new GenericData.Record(avroValueSchema);
                    thisKeyRecord.put("key", Integer.toString(total_read_lines));
                    thisValueRecord.put("Invoice", record[0]);
                    thisValueRecord.put("StockCode", record[1]);
                    thisValueRecord.put("Description", record[2]);
                    thisValueRecord.put("Quantity", Integer.parseInt(record[3]));
                    thisValueRecord.put("InvoiceDate", record[4]);
                    thisValueRecord.put("Price", Float.parseFloat(record[5]));
                    thisValueRecord.put("CustomerID", record[6]);
                    thisValueRecord.put("Country", record[7]);
                    log.info("Invoice:{}, StockCode:{}, Description:{}, Quantity:{}, InvoiceDate:{}, Price:{},CustomerID:{},Country:{}",
                    thisValueRecord.get("Invoice"),
                    thisValueRecord.get("StockCode"),
                    thisValueRecord.get("Description"),
                    thisValueRecord.get("Quantity"),
                    thisValueRecord.get("InvoiceDate"),
                    thisValueRecord.get("Price"),
                    thisValueRecord.get("CustomerID"),
                    thisValueRecord.get("Country"));
                    producer.send(new ProducerRecord<>(KAFKA_TOPIC, thisKeyRecord, thisValueRecord), postSender);
                    total_read_lines ++;
                }

            // Wait for sends to complete.
            requestLatch.await();
                
        } catch (IOException|CsvBadConverterException|InterruptedException ex) {
            ex.printStackTrace();
        }

        // Stop the thread that periodically reports progress.
        scheduler.shutdown();
        // shutdown producer
        producer.close();
    }
}
